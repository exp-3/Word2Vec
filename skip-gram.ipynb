{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5e/a51a5df287753c69459ca4572ecd9db78a259007734c4e19af7c5d68080c/tensorflow_gpu-1.2.0-cp35-cp35m-manylinux1_x86_64.whl (89.2MB)\n",
      "\u001b[K    99% |████████████████████████████████| 89.1MB 11.8MB/s eta 0:00:01    19% |██████▏                         | 17.2MB 11.8MB/s eta 0:00:07    58% |██████████████████▋             | 51.9MB 12.1MB/s eta 0:00:04^C\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines :  49199\n",
      "vocabulary number is :  9999\n",
      "train data :  3851436\n",
      "epoch 0 | Train loss 7.39313\n",
      "epoch 1 | Train loss 4.18448\n",
      "epoch 2 | Train loss 4.11805\n",
      "epoch 3 | Train loss 4.07445\n",
      "epoch 4 | Train loss 4.04044\n",
      "epoch 5 | Train loss 4.01317\n",
      "epoch 6 | Train loss 3.98961\n",
      "epoch 7 | Train loss 3.97006\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import math\n",
    "import shutil\n",
    "import glob\n",
    "from collections import Counter\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from test_word2vec import ans_make_pair\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "file_list = ['data/ptb.train.txt', 'data/ptb.valid.txt', 'data/ptb.test.txt']\n",
    "all_lines = []\n",
    "for file in file_list:\n",
    "    f=open(file)\n",
    "    all_lines.extend(f.readlines())\n",
    "    f.close()\n",
    "    \n",
    "data =[]\n",
    "for line in all_lines:\n",
    "    line = line.strip('\\n').lower().split()\n",
    "    data.append(line)\n",
    "print('lines : ', len(data))\n",
    "\n",
    "min_freq = 5\n",
    "\n",
    "words = []\n",
    "for sentence in data:\n",
    "    words.extend(sentence)\n",
    "word_cnt = Counter(words)\n",
    "\n",
    "word2id = {'<unk>':0}\n",
    "id2word = {0:'<unk>'}\n",
    "\n",
    "for word, cnt in word_cnt.most_common():\n",
    "    if cnt<min_freq:\n",
    "        break\n",
    "    if word !='<unk>':\n",
    "        word2id[word] = len(word2id)\n",
    "        id2word[len(id2word)] = word\n",
    "    \n",
    "print('vocabulary number is : ', len(word2id))\n",
    "\n",
    "converted_data = []\n",
    "for sentence in data:\n",
    "    id_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2id:\n",
    "            id_sentence.append(word2id[word])\n",
    "        else:\n",
    "            id_sentence.append(word2id['<unk>'])\n",
    "    converted_data.append(id_sentence)\n",
    "\n",
    "def make_pair(sentence, window_size):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    ## TODO\n",
    "    for target_index in range(len(sentence)):\n",
    "        for context_index in range(max(target_index - window_size, 0), min(target_index + window_size+1, len(sentence))): \n",
    "            if target_index != context_index:\n",
    "                X_train.append(sentence[target_index])\n",
    "                y_train.append(sentence[context_index])\n",
    "    ## TODO\n",
    "    return X_train, y_train\n",
    "\n",
    "skip_window = 2 # 何個隣までを予測するか\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for sentence in converted_data:\n",
    "    x, y = make_pair(sentence, skip_window)\n",
    "    X_train.extend(x)\n",
    "    y_train.extend(y)\n",
    "                \n",
    "X_train = np.array(X_train, dtype=np.int32)\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "print('train data : ', len(X_train))\n",
    "\n",
    "# モデルの保存先ディレクトリのpath\n",
    "log_path = \"./log/\"\n",
    "if os.path.exists(log_path):\n",
    "    shutil.rmtree(log_path)\n",
    "os.mkdir(log_path)\n",
    "\n",
    "model_path = os.path.join(log_path, 'model.ckpt')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "N_train = len(X_train)\n",
    "# 単語の種類の数\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "# パラメータ\n",
    "# 学習率(learning rate)\n",
    "lr = 1.0\n",
    "# 学習回数\n",
    "n_epoch = 8\n",
    "# ミニバッチサイズ\n",
    "batch_size = 128\n",
    "# word2vecのベクトルの次元\n",
    "embed_dim = 100\n",
    "# 負例をサンプリングする数\n",
    "num_sampled = 32\n",
    "\n",
    "# 入力\n",
    "x = tf.placeholder(tf.int32, shape=[None])\n",
    "y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# 単語ごとの埋め込みベクトルの一覧行列．ランダムで初期化する．\n",
    "embed_W = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0), name='word_embedding')\n",
    "\n",
    "# 単語のidから埋め込みを取得する．\n",
    "## TODO\n",
    "hidden = tf.nn.embedding_lookup(embed_W, x)\n",
    "## TODO\n",
    "\n",
    "# 埋め込み行列から出力層のネットワークのパラメータ\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocab_size, embed_dim],\n",
    "                        stddev=1.0 / math.sqrt(embed_dim)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "# 損失関数: Noise Contrastive Estimation loss\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   inputs=hidden,\n",
    "                   labels=y,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocab_size))\n",
    "\n",
    "# SGD(Stochastic Gradient Descent : 確率的勾配降下法)で目的関数を最小化する\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        print ('epoch %d | ' % epoch, end=\"\")\n",
    "\n",
    "        sum_loss = 0        \n",
    "        # 訓練データをシャッフルする\n",
    "        perm = np.random.permutation(N_train)\n",
    "                              \n",
    "        for i in range(0, N_train, batch_size):\n",
    "            # ミニバッチ分のデータを取ってくる\n",
    "            X_batch = X_train[perm[i:i+batch_size]]\n",
    "            y_batch = y_train[perm[i:i+batch_size]].reshape(-1, 1)\n",
    "        \n",
    "            feed_dict = {x:X_batch, y:y_batch}\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            sum_loss += loss_val * X_batch.shape[0]\n",
    "\n",
    "        print('Train loss %.5f' %(sum_loss/ N_train))\n",
    "        \n",
    "    # 学習されたベクトルの値を取得する\n",
    "    final_embed = embed_W.eval()\n",
    "    \n",
    "    # モデルの保存\n",
    "    saver.save(sess, model_path)\n",
    "    \n",
    "    # 埋め込み空間の可視化用 \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embed_W.name\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "    summary_writer = tf.summary.FileWriter(log_path)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    sorted_dict = sorted(word2id.items(), key=lambda x: x[1])\n",
    "    words = [\"{}\\n\".format(x[0]) for x in sorted_dict]\n",
    "    with open(\"log/metadata.tsv\", \"w\") as f:\n",
    "        f.writelines(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 近傍単語抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  kick (distance : 0.54)\n",
      "2  :  beginning (distance : 0.54)\n",
      "3  :  offices (distance : 0.55)\n",
      "4  :  similarity (distance : 0.56)\n",
      "5  :  london (distance : 0.56)\n",
      "6  :  13th (distance : 0.56)\n",
      "7  :  listed (distance : 0.57)\n",
      "8  :  shouted (distance : 0.57)\n",
      "9  :  addition (distance : 0.57)\n",
      "10  :  afternoon (distance : 0.57)\n",
      "11  :  shaking (distance : 0.57)\n",
      "12  :  cheered (distance : 0.58)\n",
      "13  :  rally (distance : 0.58)\n",
      "14  :  resignation (distance : 0.58)\n",
      "15  :  yesterday (distance : 0.58)\n",
      "16  :  nasdaq (distance : 0.58)\n",
      "17  :  rebound (distance : 0.59)\n",
      "18  :  libor (distance : 0.59)\n",
      "19  :  waves (distance : 0.59)\n",
      "20  :  rebounded (distance : 0.59)\n",
      "21  :  weakness (distance : 0.59)\n",
      "22  :  aug. (distance : 0.59)\n",
      "23  :  year-ago (distance : 0.59)\n",
      "24  :  nippon (distance : 0.59)\n",
      "25  :  gatt (distance : 0.59)\n",
      "26  :  near (distance : 0.59)\n",
      "27  :  winner (distance : 0.59)\n",
      "28  :  los (distance : 0.59)\n",
      "29  :  declines (distance : 0.6)\n",
      "30  :  defeated (distance : 0.6)\n",
      "31  :  applying (distance : 0.6)\n",
      "32  :  payroll (distance : 0.6)\n",
      "33  :  writing (distance : 0.6)\n",
      "34  :  adverse (distance : 0.6)\n",
      "35  :  watching (distance : 0.61)\n",
      "36  :  japan (distance : 0.61)\n",
      "37  :  closed (distance : 0.61)\n",
      "38  :  spite (distance : 0.61)\n",
      "39  :  powerhouse (distance : 0.61)\n",
      "40  :  tire (distance : 0.61)\n",
      "41  :  lesson (distance : 0.61)\n",
      "42  :  depositary (distance : 0.61)\n",
      "43  :  seniors (distance : 0.61)\n",
      "44  :  dealings (distance : 0.61)\n",
      "45  :  redemptions (distance : 0.61)\n",
      "46  :  october (distance : 0.61)\n",
      "47  :  chugai (distance : 0.61)\n",
      "48  :  yearly (distance : 0.62)\n",
      "49  :  b-2 (distance : 0.62)\n"
     ]
    }
   ],
   "source": [
    "# ベクトルの正規化\n",
    "norm_embed = final_embed/np.linalg.norm(final_embed, axis=1, keepdims=True)\n",
    "\n",
    "#　コサイン距離が近い単語上位top_n個取得する\n",
    "def get_sim_word(query, top_n=50):\n",
    "    query = query/np.linalg.norm(query)\n",
    "    cos = 1- np.dot(query[np.newaxis,:], norm_embed.T)[0]\n",
    "    sim = np.argsort(cos)\n",
    "    return cos[sim[:top_n]], sim[:top_n]\n",
    "\n",
    "input_word = 'tokyo'\n",
    "result = final_embed[word2id[input_word]]\n",
    "scores, indices = get_sim_word(result)\n",
    "for i, index in enumerate(indices):\n",
    "    if id2word[index]!=input_word:\n",
    "        print(i, ' : ', id2word[index], '(distance : {0:.2})'.format(scores[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
